
---
title: Hands-On-Large-Language-Models
---


# Hands-On-Large-Language-Models

> ğŸ”— **GitHub åœ°å€**: <https://github.com/HandsOnLLM/Hands-On-Large-Language-Models>

## ä¸»è¦ç‰¹æ€§

| ç‰¹æ€§ | ç®€è¿° |
|------|------|
| **å¤šæ¨¡å‹æ”¯æŒ** | åŒ…å« Llama 2ã€Mixtralã€OpenAI GPT ç³»åˆ—ç­‰ä¸»æµ LLMï¼Œæ–¹ä¾¿å¯¹æ¯”ä¸å®éªŒ |
| **æ˜“ç”¨çš„ Notebook** | Jupyter Notebook é›†æˆäº†ä»æ•°æ®é¢„å¤„ç†ã€å¾®è°ƒã€æ¨ç†åˆ°è¯„ä¼°çš„å®Œæ•´æµç¨‹ |
| **Prompt Engineering** | å¤šç§æç¤ºæ¨¡æ¿ä¸ç­–ç•¥ï¼Œç¤ºä¾‹è¦†ç›–é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ä¸å¾®è°ƒåæç¤º |
| **è®­ç»ƒä¸å¾®è°ƒ** | æä¾›åŸºäº Hugging Face Transformers çš„ fineâ€‘tuning è„šæœ¬ï¼Œæ”¯æŒ LoRAã€PEFT ç­‰æŠ€æœ¯ |
| **è¯„ä¼°ä¸åŸºå‡†** | å†…ç½®å¯¹è¯ç”Ÿæˆã€é˜…è¯»ç†è§£ã€æ¨ç†ç­‰ä»»åŠ¡çš„è¯„æµ‹è„šæœ¬ï¼Œæ”¯æŒè‡ªå®šä¹‰æ•°æ®é›† |
| **å¯æ‰©å±•çš„é¡¹ç›®ç»“æ„** | ç»Ÿä¸€çš„ `src/` ã€`data/`ã€`notebooks/` ç­‰ç›®å½•ï¼Œæ–¹ä¾¿æ’ä»¶å¼å¼€å‘ |
| **CI/CD ä¸è‡ªåŠ¨åŒ–** | GitHub Actions è‡ªåŠ¨åŒ–æµ‹è¯•ä¸æ„å»ºï¼Œç¡®ä¿ä»£ç è´¨é‡ |

## æ ¸å¿ƒåŠŸèƒ½

1. **ç¯å¢ƒæ­å»º**  
   - ä¾èµ– `requirements.txt` / `environment.yml`ï¼Œæ”¯æŒ CPU / GPU / CUDA ç‰ˆæœ¬
2. **æ¨¡å‹ä¸‹è½½ä¸ç¼“å­˜**  
   - è„šæœ¬ `scripts/download_models.py` å¯ä¸€æ¬¡æ€§ä¸‹è½½æ‰€éœ€é¢„è®­ç»ƒæƒé‡
3. **æ•°æ®é¢„å¤„ç†**  
   - `scripts/preprocess_data.py` æ”¯æŒ JSON, CSV, Markdown ç­‰å¤šç§æ ¼å¼è½¬æ¢ä¸º trainer è¾“å…¥
4. **æ¨¡å‹å¾®è°ƒ**  
   - `train_lora.py` / `train_peft.py`ï¼šé’ˆå¯¹ LLM å¾®è°ƒï¼Œæ”¯æŒ LoRAã€QLoRAã€QLoRA ç­‰
5. **æ¨ç†æ¥å£**  
   - `infer.py`ï¼šæä¾› RESTful API ä¸å‘½ä»¤è¡Œå·¥å…·ä¸¤ç§ä½¿ç”¨æ–¹å¼
6. **è¯„ä¼°å·¥å…·**  
   - `evaluate.py`ï¼šé›†æˆ BLEU, ROUGE, METEOR, GPTScore ç­‰æŒ‡æ ‡
7. **å®éªŒè®°å½•**  
   - Jupyter Notebook è‡ªåŠ¨ç”Ÿæˆå®éªŒæ—¥å¿—ï¼Œæ–¹ä¾¿å¤ç°ä¸å¯¹æ¯”

## ç”¨æ³•ç¤ºä¾‹

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/HandsOnLLM/Hands-On-Large-Language-Models.git
cd Hands-On-Large-Language-Models

# 2. åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
conda env create -f environment.yml      # æˆ–è€… pip install -r requirements.txt
conda activate llm-experiment

# 3. ä¸‹è½½æ¨¡å‹æƒé‡
python scripts/download_models.py --model llama2-13b

# 4. é¢„å¤„ç†æ•°æ®ï¼ˆå¯é€‰ï¼‰
python scripts/preprocess_data.py --input data/raw/train.json --output data/processed/train.pkl

# 5. å¾®è°ƒï¼ˆç¤ºä¾‹: LoRAï¼‰
python train_lora.py --model llama2-13b --dataset data/processed/train.pkl --output_dir checkpoints/llama2-13b-lora

# 6. æ¨ç†
python infer.py --model checkpoints/llama2-13b-lora --prompt "è¯·è§£é‡Šé‡å­çº ç¼ ã€‚"

# 7. è¯„ä¼°
python evaluate.py --model checkpoints/llama2-13b-lora --dataset data/processed/dev.pkl
```

> **æ³¨æ„**  
> - è‹¥ä½¿ç”¨ GPUï¼Œè¯·ç¡®è®¤å·²å®‰è£…å¯¹åº”çš„ CUDA é©±åŠ¨ä¸ cuDNNã€‚  
> - è¿è¡Œè¿‡ç¨‹ä¸­å¦‚é‡åˆ°æ˜¾å­˜ä¸è¶³ï¼Œå¯ä½¿ç”¨ `--low_cpu_mem_usage` æˆ–å¯ç”¨æ¢¯åº¦ç´¯ç§¯ã€‚  

---

> ä»¥ä¸Šå†…å®¹å¯ç›´æ¥å¤åˆ¶ä¿å­˜ä¸º  
> `src/content/docs/00/Hands-On-Large-Language-Models_HandsOnLLM.md`
```

ğŸ’ Support this free API: https://www.paypal.com/donate/?hosted_button_id=XS3CAYT8LE2BL