
---
title: nano-vllm
---

# nano-vllm (GeeeekExplorer)

é¡¹ç›®åœ°å€: https://github.com/GeeeekExplorer/nano-vllm

## ä¸»è¦ç‰¹æ€§
- **è½»é‡çº§**: ä¸“ä¸ºå°å‹ GPU/CPU è®¾è®¡ï¼Œæ˜¾å­˜å ç”¨ä½ï¼Œå¯åœ¨ RTX 6000 / 4090 ç­‰æ˜¾å¡ä¸Šé«˜æ•ˆè¿è¡Œã€‚  
- **å¤šæ¨¡å‹æ”¯æŒ**: å…¼å®¹ LLaMAã€ChatGLMã€Phi ç­‰å¼€æºæ¨¡å‹ï¼Œæ”¯æŒå¤šç§ç²¾åº¦ï¼ˆfp8ã€int4ã€int8 ç­‰å‹ç¼©æƒé‡ï¼‰ã€‚  
- **é«˜ååé‡**: ä¸æ ‡å‡† vLLM ç›¸æ¯”ï¼Œæ€§èƒ½æå‡å¯è¾¾ 1.5Ã—ï¼Œæ˜¾è‘—é™ä½å»¶è¿Ÿã€‚  
- **æ¨¡å‹å‹ç¼©**: è‡ªå¸¦ä½ä½å®½å‹ç¼©å­˜å‚¨ä¸ç®—å­ï¼Œæ”¯æŒæ¨¡å‹å¾®è°ƒåè‡ªåŠ¨åˆ‡æ¢åˆ°å‹ç¼©æ ¼å¼ã€‚  
- **ç»Ÿä¸€ API**: æä¾› `model.load()`, `model.generate()` ä¸ `model.stream_generate()` ç­‰ç»Ÿä¸€æ¥å£ã€‚  

## åŠŸèƒ½æ¦‚è§ˆ
1. **æ¨¡å‹åŠ è½½**ï¼šæ”¯æŒ `.bin` / `.safetensors` è¯»å–ï¼Œå‹ç¼©åç›´æ¥æ”¾å›ç£ç›˜ã€‚  
2. **æ¨ç†**ï¼šæ‰¹å¤„ç†ã€æµæ°´çº¿æ¨ç†ï¼›å¯é€šè¿‡ `max_new_tokens`, `p`, `temperature` ç­‰å‚æ•°ç»†è°ƒç”Ÿæˆè´¨é‡ã€‚  
3. **åˆ†è¯**ï¼šæ ¹æ®æ¨¡å‹ä¸åŒä½¿ç”¨åˆé€‚ tokenizerã€‚  
4. **æµå¼è¿”å›**ï¼š`stream_generate()` æä¾›æŒ‰ token é€æ­¥è¾“å‡ºï¼Œé€‚ç”¨äºèŠå¤©æœºå™¨äººã€‚  
5. **PyTorch å…¼å®¹**ï¼šå†…éƒ¨ä½¿ç”¨ PyTorch Tensorï¼Œå¹¶å¯è°ƒç”¨ `to("cuda")`ï¼Œä¿æŒä¸åŸç”Ÿ vLLM çš„ä¸€è‡´æ€§ã€‚  
6. **æ€§èƒ½è°ƒä¼˜**ï¼šæä¾› autotune çš„å‡ ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬å·ç§¯+softmax fusionã€ä½é˜¶çŸ©é˜µä¹˜äº’æ¢ã€‚  

## ç”¨æ³•ç¤ºä¾‹

```python
from nano_vllm import NanoModel, NanoTokenizer

# åˆå§‹åŒ–æ¨¡å‹
model = NanoModel(
    model_path="path/to/llama-7b-8bit",
    device="cuda:0",
    max_seq_len=4096
)

# åŠ è½½ tokenizer
tokenizer = NanoTokenizer.from_pretrained("path/to/llama-7b-8bit")

# ç”Ÿæˆæ–‡æœ¬
prompt = "è¯·è§£é‡Šä¸€ä¸‹é‡å­åŠ›å­¦çš„åŸºæœ¬åŸç†ï¼š"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)

output_ids = model.generate(
    input_ids,
    max_new_tokens=50,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(generated_text)

# æµå¼è¾“å‡º
for token_id in model.stream_generate(
    input_ids,
    max_new_tokens=50,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
):
    print(tokenizer.decode(token_id, skip_special_tokens=True), end="", flush=True)
```

## ç¯å¢ƒä¾èµ–
- Python â‰¥ 3.9  
- PyTorch â‰¥ 2.0 (CUDA 12.x / CPU)  
- transformers 4.30+ (tokenizer)  
- numba 0.55+ (ä¸ºå‹ç¼©è¿ç®—åŠ é€Ÿ)  

## å¼€å‘ä¸è´¡çŒ®
è¯·å‚é˜… repo å†…çš„ `CONTRIBUTING.md` ä¸ `docs/` ç›®å½•è·å–è¯¦ç»†çš„å¼€å‘æµç¨‹ä¸ä»£ç è§„èŒƒã€‚  

---  

> ğŸ› ï¸ **å¿«é€Ÿå¯åŠ¨**

```bash
pip install -e .
# æˆ–è€…ç›´æ¥ä» PyPI å®‰è£…
pip install nano-vllm
